{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn, context\n",
    "\n",
    "from mindnlp.transforms import PadTransform\n",
    "from mindnlp.transforms.tokenizers import BertTokenizer\n",
    "\n",
    "from mindnlp.engine import Trainer, Evaluator\n",
    "from mindnlp.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class Loader:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self._data = []  # This will store dictionaries\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.path, 'r', encoding='utf-8') as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
    "            next(spamreader, None)  # skip the headers\n",
    "            for row in spamreader:\n",
    "                res = {}\n",
    "                res['question'] = str(row[1])\n",
    "                res['answer']=str(row[5])\n",
    "                res['label'] = int(row[6])\n",
    "                self._data.append(res)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._data[index]['label'], self._data[index]['question'],self._data[index]['answer']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20347"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = Loader('ours/WikiQA-train.tsv')\n",
    "valid_file = Loader('ours/WikiQA-dev.tsv')\n",
    "test_file = Loader('ours/WikiQA-test.tsv')\n",
    "len(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(source, tokenizer, pad_value, max_seq_len=64, batch_size=32, shuffle=True):\n",
    "    column_names = [\"label\", \"question\",'answer']\n",
    "    rename_columns = [\"label\", \"input_ids\"]\n",
    "    \n",
    "    def concat_columns(data1, data2):\n",
    "        return np.concatenate((data1, data2[1:]), axis=0)  # Skip the first element of data2\n",
    "\n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "    pad_op = PadTransform(max_seq_len, pad_value=pad_value)\n",
    "    type_cast_op = transforms.TypeCast(mindspore.int32)\n",
    "    \n",
    "    # map dataset\n",
    "    dataset = dataset.map(tokenizer, input_columns=\"question\")\n",
    "    dataset = dataset.map(tokenizer, input_columns=\"answer\")\n",
    "    dataset = dataset.map(operations=[type_cast_op], input_columns=\"label\")\n",
    "\n",
    "    # Concatenate question and answer columns and then pad the result\n",
    "    dataset = dataset.map(operations=concat_columns, input_columns=[\"question\", \"answer\"], output_columns=[\"input_ids\"], column_order=[\"label\", \"input_ids\"])\n",
    "    dataset = dataset.map(operations=pad_op, input_columns=\"input_ids\")  # Apply padding\n",
    "\n",
    "    # batch dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pad_value = tokenizer.token_to_id('[PAD]')\n",
    "dataset_train = process_dataset(train_file, tokenizer, pad_value)\n",
    "dataset_val = process_dataset(valid_file, tokenizer, pad_value)\n",
    "dataset_test = process_dataset(test_file, tokenizer, pad_value, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(45341:281473510922816,MainProcess):2023-08-31-19:41:01.528.589 [/home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/abc/models/pretrained_model.py:454] The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.layer_norm.gamma', 'cls.predictions.transform.layer_norm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.models import BertForSequenceClassification\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "# set bert config and define parameters for training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-5)\n",
    "\n",
    "metric = Accuracy()\n",
    "\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='bert_qabot', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', ckpt_name='bert_qabot_best', auto_load=True)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, metrics=metric,\n",
    "                  epochs=5, loss_fn=loss, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb],\n",
    "                  jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b8a79509734c38a34336203f5e7346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.run('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "graph = mindspore.load(\"checkpoint\\bert_qabot_best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import Tensor\n",
    "\n",
    "def predict_single(question, answer):\n",
    "    label_map = {0: \"错误\", 1: \"正确\"}\n",
    "    # Tokenize and convert to numpy arrays\n",
    "    ques = tokenizer.encode(question).ids\n",
    "    ans = tokenizer.encode(answer).ids\n",
    "    \n",
    "    # Concatenate question and answer\n",
    "    text_tokenized = np.concatenate((ques, ans[1:]))\n",
    "    \n",
    "    # Convert concatenated tokens back to tensor and get prediction\n",
    "    logits = model(Tensor([text_tokenized]))\n",
    "    \n",
    "    predict_label = logits[0].asnumpy().argmax()\n",
    "    info = f\"inputs: '{question} {answer}', predict: '{label_map[predict_label]}'\"\n",
    "    \n",
    "    return info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how are glacier caves formed?'\n",
    "answer = 'A glacier cave is a cave formed within the ice of a glacier .'\n",
    "print(predict_single(question, answer))"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": "",
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "ASCEND"
  },
  "imageInfo": {
   "id": "31ae7ba4-63e6-4fa6-8aeb-cb382953e414",
   "name": "mindspore_1.10.0-cann_6.0.1-py_3.7-euler_2.8.3"
  },
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
