{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Bert微调的自动判卷系统\n",
    "本项目为基于华为云服务器Ascend: 1*Ascend910与mindspore-ascend 1.10.0的使用Bert进行全模型微调的问答机器人，其可以实现输入一句英文问题以及一个句子，从而获得该句子是否为问题答案的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验环境准备\n",
    "\n",
    "- 安装相应的库，由于git上的mindnlp库中不包含`tqdm`库，故需要单独安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://openi.pcl.ac.cn/lvyufeng/mindnlp\n",
    "!pip install regex\n",
    "!pip uninstall tqdm -y\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 导入相应的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn, context\n",
    "\n",
    "from mindnlp.transforms import PadTransform\n",
    "from mindnlp.transforms.tokenizers import BertTokenizer\n",
    "\n",
    "from mindnlp.engine import Trainer, Evaluator\n",
    "from mindnlp.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据处理\n",
    "### 2.1. 实验文件导入\n",
    "在ours文件夹下存放有`WikiQA-train.tsv`、`WikiQA-dev.tsv`、`WikiQA-test.tsv`三个tsv文件，其中包含问题Question、句子Sequence和判断是否为答案的标识Label\n",
    "\n",
    "在这里书写`Loader`类对实验内容进行加载，并返回一个字典，其中包含问题`question`、句子`answer`和判断是否为答案的标识`label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class Loader:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self._data = []  # This will store dictionaries\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        with open(self.path, 'r', encoding='utf-8') as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
    "            next(spamreader, None)  # skip the headers\n",
    "            for row in spamreader:\n",
    "                res = {}\n",
    "                res['question'] = str(row[1])\n",
    "                res['answer']=str(row[5])\n",
    "                res['label'] = int(row[6])\n",
    "                self._data.append(res)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._data[index]['label'], self._data[index]['question'],self._data[index]['answer']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20347"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = Loader('ours/WikiQA-train.tsv')\n",
    "valid_file = Loader('ours/WikiQA-dev.tsv')\n",
    "test_file = Loader('ours/WikiQA-test.tsv')\n",
    "len(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 对原始数据的处理\n",
    "由于原始数据与Bert的输入有较大区别，故在进行训练之前，需要对数据进行处理，其大体可以分为以下几个步骤：\n",
    "1. 将数据导入`mindspore.dataset.GeneratorDataset`中，以便于后续的直接处理\n",
    "2. 将question与answer进行tokenizer的处理，变成token的形式，将label转化成type_cast_op\n",
    "3. 将question与answer进行合并，生成`[CLS]question[SEP]answer[SEP]`的形式，记作`input_ids`\n",
    "4. 对`input_ids`剩下的部分用`[PAD]`进行填充，填充为`max_seq_len`的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(source, tokenizer, pad_value, max_seq_len=64, batch_size=32, shuffle=True):\n",
    "    column_names = [\"label\", \"question\",'answer']\n",
    "    rename_columns = [\"label\", \"input_ids\"]\n",
    "    \n",
    "    def concat_columns(data1, data2):\n",
    "        return np.concatenate((data1, data2[1:]), axis=0)  # Skip the first element of data2\n",
    "\n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "    pad_op = PadTransform(max_seq_len, pad_value=pad_value)\n",
    "    type_cast_op = transforms.TypeCast(mindspore.int32)\n",
    "    \n",
    "    # map dataset\n",
    "    dataset = dataset.map(tokenizer, input_columns=\"question\")\n",
    "    dataset = dataset.map(tokenizer, input_columns=\"answer\")\n",
    "    dataset = dataset.map(operations=[type_cast_op], input_columns=\"label\")\n",
    "\n",
    "    # Concatenate question and answer columns and then pad the result\n",
    "    dataset = dataset.map(operations=concat_columns, input_columns=[\"question\", \"answer\"], output_columns=[\"input_ids\"], column_order=[\"label\", \"input_ids\"])\n",
    "    dataset = dataset.map(operations=pad_op, input_columns=\"input_ids\")  # Apply padding\n",
    "\n",
    "    # batch dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "pad_value = tokenizer.token_to_id('[PAD]')\n",
    "dataset_train = process_dataset(train_file, tokenizer, pad_value)\n",
    "dataset_val = process_dataset(valid_file, tokenizer, pad_value)\n",
    "dataset_test = process_dataset(test_file, tokenizer, pad_value, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型训练\n",
    "基于预训练好的`BertForSequenceClassification`中的`bert-base-uncase`进行微调，以适配本项目的任务需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(45341:281473510922816,MainProcess):2023-08-31-19:41:01.528.589 [/home/ma-user/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindnlp/abc/models/pretrained_model.py:454] The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.layer_norm.gamma', 'cls.predictions.transform.layer_norm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.models import BertForSequenceClassification\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "# set bert config and define parameters for training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-5)\n",
    "\n",
    "metric = Accuracy()\n",
    "\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='bert_qabot', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', ckpt_name='bert_qabot_best', auto_load=True)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, metrics=metric,\n",
    "                  epochs=5, loss_fn=loss, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb],\n",
    "                  jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b8a79509734c38a34336203f5e7346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.run('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "graph = mindspore.load(\"checkpoint\\bert_qabot_best.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型推理\n",
    "对于未知的问题及解答（英文）使用训练好的模型进行推理，得到答案是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import Tensor\n",
    "\n",
    "def predict_single(question, answer):\n",
    "    label_map = {0: \"错误\", 1: \"正确\"}\n",
    "    # Tokenize and convert to numpy arrays\n",
    "    ques = tokenizer.encode(question).ids\n",
    "    ans = tokenizer.encode(answer).ids\n",
    "    \n",
    "    # Concatenate question and answer\n",
    "    text_tokenized = np.concatenate((ques, ans[1:]))\n",
    "    \n",
    "    # Convert concatenated tokens back to tensor and get prediction\n",
    "    logits = model(Tensor([text_tokenized]))\n",
    "    \n",
    "    predict_label = logits[0].asnumpy().argmax()\n",
    "    info = f\"inputs: '{question} {answer}', predict: '{label_map[predict_label]}'\"\n",
    "    \n",
    "    return info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how are glacier caves formed?'\n",
    "answer = 'A glacier cave is a cave formed within the ice of a glacier .'\n",
    "print(predict_single(question, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据集的限制，模型在推理未知的问题上精确度有所缺失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "- 基于Bert实现知识库问答：[https://work.datafountain.cn/forum?id=121&type=2&source=1](https://work.datafountain.cn/forum?id=121&type=2&source=1)\n",
    "- MindNLP开源地址：h[ttps://openi.pcl.ac.cn/lvyufeng/mindnlp](https://openi.pcl.ac.cn/lvyufeng/mindnlp)\n",
    "- MindNLP文档：[https://mindnlp.cqu.ai/en/latest/](https://mindnlp.cqu.ai/en/latest/)\n",
    "- 基于GPT2与mindspore的总结项目：[https://github.com/mindspore-lab/mindnlp/blob/master/examples/summarization/gpt2_summarization.ipynb](https://github.com/mindspore-lab/mindnlp/blob/master/examples/summarization/gpt2_summarization.ipynb)\n",
    "- 基于Bert与mindnlp的情绪分类任务：[https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=e486c037-76ae-415b-90a7-7766ea189982](https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=e486c037-76ae-415b-90a7-7766ea189982)"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": "",
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "ASCEND"
  },
  "imageInfo": {
   "id": "31ae7ba4-63e6-4fa6-8aeb-cb382953e414",
   "name": "mindspore_1.10.0-cann_6.0.1-py_3.7-euler_2.8.3"
  },
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
